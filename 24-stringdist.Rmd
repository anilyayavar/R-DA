# Finding string similarity 

Comparison of two (or more) numeric fields is an easy job in the sense that we can use multiple statistical methods available to measure comparison between these.  On the other hand, comparing strings in any way, shape or form is not a trivial task.  Despite this complexity, comparing text strings is a common and fundamental task in many text-processing algorithms.  Basic objective of all string similarity algorithms are to quantify the similarity between two text strings in terms of string metrics.   

The fuzzy matching problems are to input two strings and return a score quantifying the likelihood that they are expressions of the same entity. So (`Geeta` and `Gita`) should get a high score but not (`Apple` and `Microsoft`).  Over several decades, various algorithms for fuzzy string matching have emerged. They have varying strengths and weaknesses. These fall into two broad categories: `lexical matching` and `phonetic matching`.

## Lexical matching
*Lexical matching algorithms* match two strings based on some model of errors. Typically they are meant to match strings that differ due to spelling or typing errors. Consider `Atharv` and `ahtarv`. A lexical matching algorithm would pick up that `ht` is a transposition of `th`.  Such transposition errors are common. Given this, and that the rest of the two strings match exactly and are long enough, we should score this match as high. 

Normally, algorithms to find lexical matching, can be classified into 'edit distance based' or 'token based'.

### Levenshtein algorithm
It is named after *Vladimir Levenshtein*, who considered this distance in 1965. The `Levenshtein distance` between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other.  Levenshtein distance may also be referred to as *edit distance*, although it may also denote a larger family of distance metrics. It is closely related to pairwise string alignments.  

For the two words `helo` and `hello`, it is obvious that there is a missing character `"l"`. Thus to transform the word `helo` to `hello` all we need to do is insert that character. The distance, in this case, is `1` because there is only one edit needed.

### Hamming distance
This distance is computed by overlaying one string over another and finding the places where the strings vary. Note, classical implementation was meant to handle strings of same length. Some implementations may bypass this by adding a padding at prefix or suffix. Nevertheless, the logic is to find the total number of places one string is different from the other. 

### Jaro-Winkler
This algorithms gives high scores to two strings if, 

1. they contain same characters, but within a certain distance from one another, and 
2. the order of the matching characters is same. 

To be exact, the distance of finding similar character is 1 less than half of length of longest string. So if longest strings has length of 5, a character at the start of the string 1 must be found before or on ((5/2)â€“1) ~ 2nd position in the string 2 to be considered valid match. Because of this, the algorithm is directional and gives high score if matching is from the beginning of the strings. 

### N-Gram

### Jaccard's Index

## Phonetic matching
*Phonetic matching algorithms* match strings based on how similar they sound. Consider `Geeta` and `Gita.` They sound similar enough that one person might spell as `Geetha` or `Geeta`, another as `Gita.` As in this case, one is not necessarily a misspelling of the other. just sounds similar.

